model:
  base_learning_rate: 1e-6
  target: specvqgan.models.cond_transformer.Net2NetTransformer
  params:
    cond_stage_key: feature
    transformer_config:
      target: specvqgan.modules.transformer.mingpt.GPTFeats
      params:
        feat_embedding_config:
          target: torch.nn.Conv1d
          params:
            in_channels: 512  # feat_depth, the input text feature is 768,clip is 512
            out_channels: 1024  # n_embd
            kernel_size: 1
            padding: 0
        GPT_config:
          vocab_size: 2048 # the size of codebook,
          block_size: 266  # 53*5 + 1
          n_layer: 19
          n_head: 16
          n_embd: 1024
    first_stage_permuter_config:
      target: specvqgan.modules.transformer.permuter.ColumnMajor
      params:
        H: 5  # mel_num, num of feats in specs / down_factor
        W: 53  # cropped spec length / down_factor
    first_stage_config:
      target: specvqgan.models.vqgan.VQModel
      params:
        ckpt_path: 'Codebook/logs/2022-04-20T16-47-17_codebook_finetune_caps/checkpoints/last.ckpt' # e.g. '2021-06-06T19-42-53_vas_specs_vqgan' or '2021-05-19T22-16-54_vggsound_specs_vqgan'
        embed_dim: 256
        n_embed: 2048 # 1024 for vggsound codebooks, 256 for caps codebooks
        ddconfig:
          double_z: false
          z_channels: 256
          resolution: 848
          in_channels: 1
          out_ch: 1
          ch: 128
          ch_mult: [1, 1, 2, 2, 4]
          num_res_blocks: 2
          attn_resolutions: [53]
          dropout: 0.0
        lossconfig:
          target: specvqgan.modules.losses.DummyLoss
    # no permuter for the cond stage as the raw features is already a sequence
    cond_stage_config:
      target: specvqgan.modules.misc.raw_feats.RawFeatsStage

lightning:
  modelcheckpoint:
    target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      monitor: val/loss
      mode: min
      save_last:
  callbacks:
    image_logger:
      target: train.ImageLogger
      params:
        for_specs: True
        vocoder_cfg:
          target: train.VocoderMelGan
          params:
            ckpt_vocoder: 'Codebook/vocoder_audioset/logs/audioset/'
    early_stop_callback:
      target: pytorch_lightning.callbacks.EarlyStopping
      params:
        monitor: val/loss
        mode: min
        min_delta: 0.00
        patience: 2
        verbose: True



data:
  target: train.ConditionedSpectrogramDataModuleFromConfig_caps
  params:
    batch_size: 20
    num_workers: 8

    spec_dir_path: 'Codebook/data/audiocaps/features/*/melspec_10s_22050hz'
    sample_rate: 22050
    mel_num: 80
    spec_len: 860
    spec_crop_len: 848
    random_crop: False

    cls_token_dir_path: 'Codebook/data/audiocaps/clip_text/*/cls_token_512' # BNInception features
    # rgb_feats_dir_path: './data/vas/features/*/feature_resnet50_dim2048_21.5fps' # ResNet50 features
    feat_depth: 512 # 768 for bert,512 for clip
    feat_len: 1 # previous value is 215
    feat_crop_len: 1 # previous value is 212
    feat_sampler_cfg:
      target: specvqgan.data.vas.ResampleFrames
      params:
        feat_sample_size: 1  # will resample `feat_crop_len` evenly to match `feat_sample_size`
        # times_to_repeat_after_resample: 5  # will duplicate each of `feat_sample_size` by this number

    train:
      target: specvqgan.data.caps.VASSpecsCondOnFeatsTrain
      params:
        specs_dataset_cfg:
          split_path: 'Codebook/data/audiocaps/txt/caps_train.txt'
        condition_dataset_cfg:
          split_path: 'Codebook/data/audiocaps/txt/caps_train.txt'
    validation:
      target: specvqgan.data.caps.VASSpecsCondOnFeatsValidation
      params:
        specs_dataset_cfg:
          split_path: 'Codebook/data/audiocaps/txt/caps_valid.txt'
        condition_dataset_cfg:
          split_path: 'Codebook/data/audiocaps/txt/caps_valid.txt'
